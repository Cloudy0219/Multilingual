{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import random as rn\n",
    "import datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Twitter Data:\n",
      "Dataset size: 9491 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9491 entries, 0 to 9490\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   document  9491 non-null   object \n",
      " 1   labels    9491 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 148.4+ KB\n",
      "====================================================================================================\n",
      "Load Reddit Data:\n",
      "Dataset size: 1797 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1797 entries, 0 to 1796\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   document  1797 non-null   object \n",
      " 1   labels    1797 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 28.2+ KB\n",
      "====================================================================================================\n",
      "Combine Twitter&Reddit Data:'\n",
      "Dataset size: 11288 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11288 entries, 0 to 1796\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   document  11288 non-null  object \n",
      " 1   labels    11288 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 264.6+ KB\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Twitter Dataset\n",
    "print(\"Load Twitter Data:\")\n",
    "twitter_df_train = pd.read_csv('data/twitter_train.csv', on_bad_lines='skip')\n",
    "twitter_df_train = twitter_df_train.drop(columns=['language'])\n",
    "twitter_df_train = twitter_df_train.rename(columns={'text': 'document', 'label': 'labels'})\n",
    "print(\"Dataset size:\", len(twitter_df_train), '\\n')\n",
    "twitter_df_train.info()\n",
    "print(\"==\"*50)\n",
    "\n",
    "# Load Reddit Dataset\n",
    "print(\"Load Reddit Data:\")\n",
    "reddit_df_train = pd.read_csv('data/annotated_question_intimacy_data/final_train.txt',\n",
    "                              sep='\\t', header=None, names=['document', 'labels'])\n",
    "print(\"Dataset size:\", len(reddit_df_train), '\\n')\n",
    "reddit_df_train.info()\n",
    "print(\"==\"*50)\n",
    "\n",
    "# Combine Data\n",
    "print(\"Combine Twitter&Reddit Data:'\")\n",
    "combined_df = pd.concat([twitter_df_train,reddit_df_train])\n",
    "print(\"Dataset size:\", len(combined_df), '\\n')\n",
    "combined_df.info()\n",
    "print(\"==\"*50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            document  labels\n0  What are the most mediocre animals in the anim...     2.3\n1  What's the difference between an allergic reac...     3.1\n2  What is your favorite subreddit that not every...     3.1\n3  What's the most disgusting meal you've ever ea...     3.5\n4           Whats one question you hate being asked?     4.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What are the most mediocre animals in the anim...</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What's the difference between an allergic reac...</td>\n      <td>3.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is your favorite subreddit that not every...</td>\n      <td>3.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What's the most disgusting meal you've ever ea...</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Whats one question you hate being asked?</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Transformation for Reddit\n",
    "A, B, C, D = -1, 1, 1, 5\n",
    "scale = (D-C)/(B-A)\n",
    "offset = -A*(D-C)/(B-A) + C\n",
    "\n",
    "for index, row in reddit_df_train.iterrows():\n",
    "  iScore = row['labels']\n",
    "\n",
    "  # If the cell is re-run without clearing local variables, we'll\n",
    "  # double convert the values between the 1-5 range resulting in values between\n",
    "  # 5-10. This condition makes sure original scores from Reddit are not already\n",
    "  #  greater than 1.\n",
    "  if iScore > 1:\n",
    "    break\n",
    "\n",
    "  q = iScore * scale + offset\n",
    "  reddit_df_train.at[index, 'labels'] = round(q, 1)\n",
    "\n",
    "reddit_df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process to Dataset for Huggingface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['document', 'labels', '__index_level_0__'],\n        num_rows: 9030\n    })\n    test: Dataset({\n        features: ['document', 'labels', '__index_level_0__'],\n        num_rows: 2258\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_dataset = Dataset.from_pandas(twitter_df_train)\n",
    "twitter_dataset = twitter_dataset.train_test_split(test_size=0.2)\n",
    "twitter_dataset\n",
    "\n",
    "reddit_dataset = Dataset.from_pandas(reddit_df_train)\n",
    "reddit_dataset = reddit_dataset.train_test_split(test_size=0.2)\n",
    "reddit_dataset\n",
    "\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "combined_dataset = combined_dataset.train_test_split(test_size=0.2)\n",
    "combined_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\", cache=\"/Users/qinyuanzheng/huggingface_cache/models\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sentence = \"If you trust them they will always be here for us too üíïüíïüíï\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅIf', '‚ñÅyou', '‚ñÅtrust', '‚ñÅthem', '‚ñÅthey', '‚ñÅwill', '‚ñÅalways', '‚ñÅbe', '‚ñÅhere', '‚ñÅfor', '‚ñÅus', '‚ñÅtoo', '‚ñÅ', 'üíï', 'üíï', 'üíï']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4263, 398, 63207, 2856, 1836, 1221, 11343, 186, 3688, 100, 1821, 5792, 6, 178556, 178556, 178556]\n"
     ]
    }
   ],
   "source": [
    "token_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e402fd515ad14c719e6f1e1fb4a1dbc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fbf8b3ee3b441b0aa738877c553822b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25e10dd1cf0842ae89e6a24c1de61662"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6ab655bd2a7435c826012c160fe0df7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['document', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 1437\n    })\n    test: Dataset({\n        features: ['document', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 360\n    })\n})"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "twitter_tokenized_datasets = twitter_dataset.map(tokenize_function, batched=True)\n",
    "twitter_tokenized_datasets\n",
    "reddit_tokenized_datasets = reddit_dataset.map(tokenize_function, batched=True)\n",
    "reddit_tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Selecting a small sample of rows from training and testing datasets will help the\n",
    "# model train quickly.\n",
    "twitter_tokenized_datasets_train = twitter_tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "twitter_tokenized_datasets_test = twitter_tokenized_datasets[\"test\"].shuffle(seed=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899\n",
      "7592\n"
     ]
    }
   ],
   "source": [
    "print(len(twitter_tokenized_datasets_test))\n",
    "print(len(twitter_tokenized_datasets_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lang_dict = {\n",
    "    \"Spanish\": \"es\",\n",
    "    \"English\": \"en\",\n",
    "    \"Chinese\": \"zh\",\n",
    "    \"French\": \"fr\",\n",
    "    \"Italian\": \"it\",\n",
    "    \"Portuguese\": \"pt\",\n",
    "    \"Korean\": \"ko\",\n",
    "    \"Dutch\": \"nl\",\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"Arabic\": \"ar\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
